{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный бустинг своими руками\n",
    "\n",
    "**Внимание:** в тексте задания произошли изменения - поменялось число деревьев (теперь 50), правило изменения величины шага в задании 3 и добавился параметр `random_state` у решающего дерева. Правильные ответы не поменялись, но теперь их проще получить. Также исправлена опечатка в функции `gbm_predict`.\n",
    "\n",
    "В этом задании будет использоваться датасет `boston` из `sklearn.datasets`. Оставьте последние 25% объектов для контроля качества, разделив `X` и `y` на `X_train`, `y_train` и `X_test`, `y_test`.\n",
    "\n",
    "Целью задания будет реализовать простой вариант градиентного бустинга над регрессионными деревьями для случая квадратичной функции потерь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleksei/.virtualenvs/plots/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import cross_validation, tree, datasets, metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1\n",
    "\n",
    "Как вы уже знаете из лекций, **бустинг** - это метод построения композиций базовых алгоритмов с помощью последовательного добавления к текущей композиции нового алгоритма с некоторым коэффициентом. \n",
    "\n",
    "Градиентный бустинг обучает каждый новый алгоритм так, чтобы он приближал антиградиент ошибки по ответам композиции на обучающей выборке. Аналогично минимизации функций методом градиентного спуска, в градиентном бустинге мы подправляем композицию, изменяя алгоритм в направлении антиградиента ошибки.\n",
    "\n",
    "Воспользуйтесь формулой из лекций, задающей ответы на обучающей выборке, на которые нужно обучать новый алгоритм (фактически это лишь чуть более подробно расписанный градиент от ошибки), и получите частный ее случай, если функция потерь `L` - квадрат отклонения ответа композиции `a(x)` от правильного ответа `y` на данном `x`.\n",
    "\n",
    "Если вы давно не считали производную самостоятельно, вам поможет таблица производных элементарных функций (которую несложно найти в интернете) и правило дифференцирования сложной функции. После дифференцирования квадрата у вас возникнет множитель 2 — т.к. нам все равно предстоит выбирать коэффициент, с которым будет добавлен новый базовый алгоритм, проигноируйте этот множитель при дальнейшем построении алгоритма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_algorithms_list = []\n",
    "coefficients_list = []\n",
    "\n",
    "dataset = datasets.load_boston()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbm_predict(X):\n",
    "    \"\"\"\n",
    "    Предсказывает значения из X с помощью композиции алгоритмов\n",
    "    \"\"\"\n",
    "    return [sum([coeff * algo.predict([x])[0] for algo, coeff in zip(base_algorithms_list, coefficients_list)]) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_ans(number: int, ans):\n",
    "    with open(\"ans{}\".format(number), 'w') as f:\n",
    "        f.write(str(ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2\n",
    "\n",
    "Заведите массив для объектов `DecisionTreeRegressor` (будем их использовать в качестве базовых алгоритмов) и для вещественных чисел (это будут коэффициенты перед базовыми алгоритмами). \n",
    "\n",
    "В цикле от обучите последовательно 50 решающих деревьев с параметрами `max_depth=5` и `random_state=42` (остальные параметры - по умолчанию). В бустинге зачастую используются сотни и тысячи деревьев, но мы ограничимся 50, чтобы алгоритм работал быстрее, и его было проще отлаживать (т.к. цель задания разобраться, как работает метод). Каждое дерево должно обучаться на одном и том же множестве объектов, но ответы, которые учится прогнозировать дерево, будут меняться в соответствие с полученным в задании 1 правилом. \n",
    "\n",
    "Попробуйте для начала всегда брать коэффициент равным 0.9. Обычно оправдано выбирать коэффициент значительно меньшим - порядка 0.05 или 0.1, но т.к. в нашем учебном примере на стандартном датасете будет всего 50 деревьев, возьмем для начала шаг побольше.\n",
    "\n",
    "В процессе реализации обучения вам потребуется функция, которая будет вычислять прогноз построенной на данный момент композиции деревьев на выборке `X`:\n",
    "\n",
    "```\n",
    "def gbm_predict(X):\n",
    "    return [sum([coeff * algo.predict([x])[0] for algo, coeff in zip(base_algorithms_list, coefficients_list)]) for x in X]\n",
    "(считаем, что base_algorithms_list - список с базовыми алгоритмами, coefficients_list - список с коэффициентами перед алгоритмами)\n",
    "```\n",
    "\n",
    "Эта же функция поможет вам получить прогноз на контрольной выборке и оценить качество работы вашего алгоритма с помощью `mean_squared_error` в `sklearn.metrics`. \n",
    "\n",
    "Возведите результат в степень 0.5, чтобы получить `RMSE`. Полученное значение `RMSE` — **ответ в пункте 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_algorithms_list = []\n",
    "coefficients_list = []\n",
    "\n",
    "# Zero prediction class\n",
    "class A_0:\n",
    "    def fit(self, X, y):\n",
    "        self.n_features = X.size\n",
    "        self.n_targets = y.size\n",
    "    def predict(self, X):\n",
    "        return np.zeros(self.n_targets)\n",
    "    \n",
    "# Zero iteration\n",
    "a_0 = A_0()\n",
    "a_0.fit(X_train, y_train)\n",
    "base_algorithms_list.append(a_0)\n",
    "coefficients_list.append(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teach_model(decrease_coeffs=False):\n",
    "    for N in range(1, 51):\n",
    "        s = -(np.array(gbm_predict(X_train)) - y_train)\n",
    "        b_N = tree.DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "        b_N.fit(X_train, s)\n",
    "        base_algorithms_list.append(b_N)\n",
    "        if not decrease_coeffs:\n",
    "            coefficients_list.append(0.9)   # 0.9 coeff for all\n",
    "        else:\n",
    "            coefficients_list.append(0.9/(1.0 + N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.5 s, sys: 4 ms, total: 26.6 s\n",
      "Wall time: 26.5 s\n"
     ]
    }
   ],
   "source": [
    "%time teach_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE was 5.476650974168953\n"
     ]
    }
   ],
   "source": [
    "ans2 = metrics.mean_squared_error(gbm_predict(X_test), y_test)**0.5\n",
    "print(\"RMSE was {}\".format(ans2))\n",
    "write_ans(2, ans2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3\n",
    "\n",
    "Вас может также беспокоить, что двигаясь с постоянным шагом, вблизи минимума ошибки ответы на обучающей выборке меняются слишком резко, перескакивая через минимум. \n",
    "\n",
    "Попробуйте уменьшать вес перед каждым алгоритмом с каждой следующей итерацией по формуле `0.9 / (1.0 + i)`, где `i` - номер итерации (от 0 до 49). Используйте качество работы алгоритма как **ответ в пункте 3**. \n",
    "\n",
    "В реальности часто применяется следующая стратегия выбора шага: как только выбран алгоритм, подберем коэффициент перед ним численным методом оптимизации таким образом, чтобы отклонение от правильных ответов было минимальным. Мы не будем предлагать вам реализовать это для выполнения задания, но рекомендуем попробовать разобраться с такой стратегией и реализовать ее при случае для себя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 22s, sys: 52 ms, total: 1min 22s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%time teach_model(decrease_coeffs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE was 5.4766495845979914\n"
     ]
    }
   ],
   "source": [
    "ans3 = metrics.mean_squared_error(gbm_predict(X_test), y_test)**0.5\n",
    "print(\"RMSE was {}\".format(ans3))\n",
    "write_ans(3, ans3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4\n",
    "\n",
    "Реализованный вами метод - градиентный бустинг над деревьями - очень популярен в машинном обучении. Он представлен как в самой библиотеке `sklearn`, так и в сторонней библиотеке `XGBoost`, которая имеет свой питоновский интерфейс. На практике `XGBoost` работает заметно лучше `GradientBoostingRegressor` из `sklearn`, но для этого задания вы можете использовать любую реализацию. \n",
    "\n",
    "Исследуйте, переобучается ли градиентный бустинг с ростом числа итераций (и подумайте, почему), а также с ростом глубины деревьев. На основе наблюдений выпишите через пробел номера правильных из приведенных ниже утверждений в порядке возрастания номера (это будет **ответ в п.4**):\n",
    "\n",
    "    1. С увеличением числа деревьев, начиная с некоторого момента, качество работы градиентного бустинга не меняется существенно.\n",
    "\n",
    "    2. С увеличением числа деревьев, начиная с некоторого момента, градиентный бустинг начинает переобучаться.\n",
    "\n",
    "    3. С ростом глубины деревьев, начиная с некоторого момента, качество работы градиентного бустинга на тестовой выборке начинает ухудшаться.\n",
    "\n",
    "    4. С ростом глубины деревьев, начиная с некоторого момента, качество работы градиентного бустинга перестает существенно изменяться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleksei/.virtualenvs/plots/lib/python3.5/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn import grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = xgb.XGBRegressor(n_jobs=8)\n",
    "regressor.get_params().keys()\n",
    "grid = {\n",
    "    'n_estimators': [10, 50, 100, 1000],\n",
    "    'learning_rate': np.hstack((np.linspace(0.01, 0.1, 10), [0.9])),\n",
    "    'max_depth': [1, 2, 5, 10, 100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv = grid_search.GridSearchCV(regressor, grid, scoring='neg_mean_absolute_error', cv = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 25s, sys: 2.04 s, total: 7min 27s\n",
      "Wall time: 57.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [10, 50, 100, 1000], 'max_depth': [1, 2, 5, 10, 100], 'learning_rate': array([ 0.01,  0.02,  0.03,  0.04,  0.05,  0.06,  0.07,  0.08,  0.09,\n",
       "        0.1 ,  0.9 ])},\n",
       "       pre_dispatch='2*n_jobs', refit=True,\n",
       "       scoring='neg_mean_absolute_error', verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time grid_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.070000000000000007,\n",
      "       max_delta_step=0, max_depth=5, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=8, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n"
     ]
    }
   ],
   "source": [
    "print(grid_cv.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0775433379880495"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.mean_squared_error(grid_cv.predict(X_test), y_test)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressor:\n",
      " n_estimators is 10\n",
      " depth is 1\n",
      " MSE is 26.884424633275902\n",
      "Regressor:\n",
      " n_estimators is 10\n",
      " depth is 2\n",
      " MSE is 26.979710878069866\n",
      "Regressor:\n",
      " n_estimators is 10\n",
      " depth is 5\n",
      " MSE is 28.305789513972115\n",
      "Regressor:\n",
      " n_estimators is 10\n",
      " depth is 10\n",
      " MSE is 28.367223723407577\n",
      "Regressor:\n",
      " n_estimators is 10\n",
      " depth is 100\n",
      " MSE is 28.367223723407577\n",
      "Regressor:\n",
      " n_estimators is 50\n",
      " depth is 1\n",
      " MSE is 40.201523385516744\n",
      "Regressor:\n",
      " n_estimators is 50\n",
      " depth is 2\n",
      " MSE is 26.559115126129083\n",
      "Regressor:\n",
      " n_estimators is 50\n",
      " depth is 5\n",
      " MSE is 24.267424605727427\n",
      "Regressor:\n",
      " n_estimators is 50\n",
      " depth is 10\n",
      " MSE is 26.137867546568163\n",
      "Regressor:\n",
      " n_estimators is 50\n",
      " depth is 100\n",
      " MSE is 25.324815651627866\n",
      "Regressor:\n",
      " n_estimators is 100\n",
      " depth is 1\n",
      " MSE is 34.1719010687726\n",
      "Regressor:\n",
      " n_estimators is 100\n",
      " depth is 2\n",
      " MSE is 26.96708376323875\n",
      "Regressor:\n",
      " n_estimators is 100\n",
      " depth is 5\n",
      " MSE is 23.842264717606273\n",
      "Regressor:\n",
      " n_estimators is 100\n",
      " depth is 10\n",
      " MSE is 27.525827877184835\n",
      "Regressor:\n",
      " n_estimators is 100\n",
      " depth is 100\n",
      " MSE is 26.595250332440735\n",
      "Regressor:\n",
      " n_estimators is 1000\n",
      " depth is 1\n",
      " MSE is 49.30330687699797\n",
      "Regressor:\n",
      " n_estimators is 1000\n",
      " depth is 2\n",
      " MSE is 24.076691107747664\n",
      "Regressor:\n",
      " n_estimators is 1000\n",
      " depth is 5\n",
      " MSE is 23.971626830281743\n",
      "Regressor:\n",
      " n_estimators is 1000\n",
      " depth is 10\n",
      " MSE is 27.67909788058921\n",
      "Regressor:\n",
      " n_estimators is 1000\n",
      " depth is 100\n",
      " MSE is 26.6287428361916\n"
     ]
    }
   ],
   "source": [
    "for n_est in [10, 50, 100, 1000]:\n",
    "    for depth in [1, 2, 5, 10, 100]:\n",
    "        regressor1 = xgb.XGBRegressor(max_depth=depth, n_estimators=n_est, n_jobs=8)\n",
    "        regressor1.fit(X_train, y_train)\n",
    "        error = metrics.mean_squared_error(regressor1.predict(X_test), y_test)\n",
    "        print(\"Regressor:\\n n_estimators is {}\\n depth is {}\\n MSE is {}\"\n",
    "              .format(n_est, depth, error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5\n",
    "\n",
    "Сравните получаемое с помощью градиентного бустинга качество с качеством работы линейной регрессии. \n",
    "\n",
    "Для этого обучите `LinearRegression` из `sklearn.linear_model` (с параметрами по умолчанию) на обучающей выборке и оцените для прогнозов полученного алгоритма на тестовой выборке `RMSE`. Полученное качество - ответ в **пункте 5**. \n",
    "\n",
    "В данном примере качество работы простой модели должно было оказаться хуже, но не стоит забывать, что так бывает не всегда. В заданиях к этому курсу вы еще встретите пример обратной ситуации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lin. regression error is 8.270468034938046\n"
     ]
    }
   ],
   "source": [
    "lin_reg = linear_model.LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "ans5 = metrics.mean_squared_error(lin_reg.predict(X_test), y_test)**0.5\n",
    "print(\"Lin. regression error is {}\".format(ans5))\n",
    "write_ans(5, ans5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
